{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_04_GloveEmbedding",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B0wh2umkr1C"
      },
      "source": [
        "In this Notebook we will demontrate text classification model using Glove embeddings with Neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMC1Nv-hk6_U"
      },
      "source": [
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import sys \r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\r\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.initializers import Constant \r\n",
        "import zipfile\r\n",
        "import os \r\n",
        "import nltk\r\n",
        "import tensorflow as tf\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import regex as re \r\n",
        "from gensim.parsing.preprocessing import remove_stopwords"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlWzQNSUksa-"
      },
      "source": [
        "! wget https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter02/heart_disease.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAIzPz-amk8D"
      },
      "source": [
        "! wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZxr7pUcpERz"
      },
      "source": [
        "with zipfile.ZipFile('/content/glove.6B.zip', 'r') as zip_ref:\r\n",
        "    zip_ref.extractall('/content/Glove')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIU73Tk9gUhD"
      },
      "source": [
        "train_df = pd.read_csv('/content/IMDB Dataset.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEKwJc-SSuF1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSBOa0MDoWKE",
        "outputId": "f41fb58b-3a62-42d0-bebb-2405d7b6e1ad"
      },
      "source": [
        "train_df.shape[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0QHyqcbgSr0"
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \r\n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \r\n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \r\n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \r\n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "  text = str(text)\r\n",
        "  for punc in puncts:\r\n",
        "      if punc in text:\r\n",
        "          text = text.replace(punc, ' ')\r\n",
        "  return text\r\n",
        "\r\n",
        "def remove_emoji(text):\r\n",
        "    emoji_pattern = re.compile(\"[\"\r\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           u\"\\U00002702-\\U000027B0\"\r\n",
        "                           u\"\\U000024C2-\\U0001F251\"\r\n",
        "                           \"]+\", flags=re.UNICODE)\r\n",
        "    return emoji_pattern.sub(r'', text)\r\n",
        "\r\n",
        "#Removing the html strips\r\n",
        "def strip_html(text):\r\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\r\n",
        "    return soup.get_text()\r\n",
        "\r\n",
        "#Removing the square brackets\r\n",
        "def remove_between_square_brackets(text):\r\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\r\n",
        "\r\n",
        "#Removing the noisy text\r\n",
        "def denoise_text(text):\r\n",
        "    text = strip_html(text)\r\n",
        "    text = remove_between_square_brackets(text)\r\n",
        "    return text\r\n",
        "\r\n",
        "#Define function for removing special characters\r\n",
        "def remove_special_characters(text, remove_digits=True):\r\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\r\n",
        "    text=re.sub(pattern,'',text)\r\n",
        "    return text\r\n",
        "\r\n",
        "#Stemming the text\r\n",
        "def simple_stemmer(text):\r\n",
        "    ps=nltk.porter.PorterStemmer()\r\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\r\n",
        "    return text\r\n",
        "#Apply function on review column\r\n",
        "\r\n",
        "train_df['review'] = train_df['review'].apply(lambda x: remove_emoji(x))\r\n",
        "train_df['review'] = train_df['review'].apply(lambda x: clean_text(x))\r\n",
        "train_df['review'] = train_df['review'].apply(lambda x: denoise_text(x))\r\n",
        "train_df['review'] = train_df['review'].apply(lambda x: remove_special_characters(x))\r\n",
        "train_df['review'] = train_df['review'].apply(lambda x: simple_stemmer(x))\r\n",
        "train_df['review'] = train_df['review'] .apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Ey-1QBg1mb"
      },
      "source": [
        "# #Preprocessing the test dataset as well\r\n",
        "# test_df['tweet'] = test_df['tweet'].apply(lambda x: remove_emoji(x)) \r\n",
        "# test_df['tweet'] = test_df['tweet'].apply(lambda x: clean_text(x)) \r\n",
        "# test_df['tweet'] = test_df['tweet'].apply(lambda x: re.sub(r'http\\S+','',x))\r\n",
        "# test_df['tweet'] = test_df['tweet'].apply(lambda x: re.sub(\"@[\\w]*\", '', x))\r\n",
        "# test_df['tweet'] = test_df['tweet'].apply(lambda x:' '.join(x.split()))\r\n",
        "# test_df['tweet'] = test_df['tweet'].apply(lambda x: remove_stopwords(x))\r\n",
        "# test_df['tweet'] = test_df['tweet'].apply(lambda x: x.lower())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Tc9leRSRq7Bj",
        "outputId": "a20aad23-e00a-41f0-89a2-54bc654b19e2"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other review ha mention that after ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonder littl product br br the film techniqu...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought thi wa a wonder way to spend time on...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basic there s a famili where a littl boy jake ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei s love in the time of money is a...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  one of the other review ha mention that after ...  positive\n",
              "1  a wonder littl product br br the film techniqu...  positive\n",
              "2  i thought thi wa a wonder way to spend time on...  positive\n",
              "3  basic there s a famili where a littl boy jake ...  negative\n",
              "4  petter mattei s love in the time of money is a...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E_j-MDSuyvX"
      },
      "source": [
        "GLOVE_DIR = '/content/Glove'\r\n",
        "\r\n",
        "MAX_LENGTH_SEQ = 1000\r\n",
        "MAX_NUM_WORDS = 20000\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "VALIDATION_SPLIT = 0.20\r\n",
        "LABELS_LEN = train_df['sentiment'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXKDBCkertMV"
      },
      "source": [
        "train_df['sentiment'] = train_df['sentiment'].map({'positive':1,'negative':0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCpEwdxZpcTh"
      },
      "source": [
        "X = train_df['review']\r\n",
        "y = train_df['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFAVUuNtwRFk"
      },
      "source": [
        "## Loading and Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Kfix88wSwc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=2018)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNz84YqMub_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33da0c3e-4e9a-4aad-f27e-2cc0b26001e9"
      },
      "source": [
        "##Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\r\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\r\n",
        "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS )\r\n",
        "tokenizer.fit_on_texts(train_df['review'])\r\n",
        "train_sequences =   tokenizer.texts_to_sequences(train_df['review'])\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 71419 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8NKOuiDvHTO"
      },
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\r\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\r\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_LENGTH_SEQ)\r\n",
        "train_labels = to_categorical(np.asarray(train_df['sentiment']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB202G0WrnND"
      },
      "source": [
        "#split the training data into a training se and validation set \r\n",
        "indices = np.arange(train_data.shape[0])\r\n",
        "np.random.shuffle(indices)\r\n",
        "train_data = train_data[indices]\r\n",
        "train_labels = train_labels[indices]\r\n",
        "num_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\r\n",
        "x_train = train_data[:-num_validation_samples]\r\n",
        "y_train = train_labels[:-num_validation_samples]\r\n",
        "x_val = train_data[-num_validation_samples:]\r\n",
        "y_val = train_labels[-num_validation_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEIbSwZra1Wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c733d8-28d8-494c-ead0-0aa9a9c50ef1"
      },
      "source": [
        "print('Preparing embedding matrix')\r\n",
        "\r\n",
        "#First build index mapping words in the embedding set\r\n",
        "#to their embedding vector \r\n",
        "\r\n",
        "embedding_index = {}\r\n",
        "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\r\n",
        "  for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "    embedding_index[word] = coefs\r\n",
        "\r\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embedding_index))\r\n",
        "print(embedding_index[\"google\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "[ 0.22575  -0.56253  -0.05156  -0.079389  1.1876   -0.48397  -0.23342\n",
            " -0.85278   0.97495  -0.33344   0.71692   0.12644   0.31962  -1.4136\n",
            " -0.57903  -0.037286 -0.0164    0.45155  -0.29005   0.52599  -0.22534\n",
            " -0.29556  -0.032407  1.5608   -0.013499 -0.064558  0.26625   0.78595\n",
            " -0.71693  -0.93025   0.80461   1.6035   -0.30602  -0.34764   0.93872\n",
            "  0.38137  -0.26743  -0.56519   0.58899  -0.14554  -0.34324   0.21291\n",
            " -0.39887   0.090042 -0.8495    0.38803  -0.5045   -0.22488   1.0644\n",
            " -0.2624    1.0334    0.06348  -0.39989   0.24236  -0.65636  -1.8107\n",
            " -0.061801  0.13795   1.1658   -0.30046  -0.50143   0.16509   0.039835\n",
            "  0.62541   0.56935   0.64125   0.21308   0.30276   0.39673   0.38973\n",
            "  0.28183   0.79481  -0.11962  -0.49598  -0.53195  -0.14897   0.51254\n",
            " -0.39208  -0.58535  -0.078509  0.81721  -0.73497  -0.68131   0.099243\n",
            " -0.87608   0.029632  0.33402  -0.14305   0.16964  -0.035178  0.39777\n",
            "  0.71769   0.25867  -0.36201   0.45698  -0.39156  -0.49343  -0.11224\n",
            "  0.29046   0.73216 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9-SHtoqcR2o"
      },
      "source": [
        "#Prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from the Glove\r\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\r\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\r\n",
        "print(embedding_matrix.shape)\r\n",
        "for word, i in word_index.items():\r\n",
        "  print(i, word)\r\n",
        "  if i > MAX_NUM_WORDS:\r\n",
        "    continue\r\n",
        "  embedding_vector = embedding_index.get(word)\r\n",
        "  if embedding_vector is not None:\r\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx1gfJ40eEbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8406d6-c5a9-4ae4-b885-6a9cf51541e6"
      },
      "source": [
        "# load these pre-trained word embeddings into an Embedding layer\r\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\r\n",
        "\r\n",
        "embedding_layer = Embedding(num_words,\r\n",
        "                            EMBEDDING_DIM,\r\n",
        "                            embeddings_initializer = Constant(embedding_matrix),\r\n",
        "                            input_length=MAX_LENGTH_SEQ,\r\n",
        "                            trainable=False)\r\n",
        "print('Preparing of embedding matrix is done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing of embedding matrix is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABTBnitxmug8"
      },
      "source": [
        "\r\n",
        "**1D CNN Model with pre-trained embedding¶** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxqTpq99mwkh"
      },
      "source": [
        "cnnmodel = Sequential()\r\n",
        "cnnmodel.add(embedding_layer)\r\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\r\n",
        "cnnmodel.add(MaxPooling1D(5))\r\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\r\n",
        "cnnmodel.add(MaxPooling1D(5))\r\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\r\n",
        "cnnmodel.add(GlobalMaxPooling1D())\r\n",
        "cnnmodel.add(Dense(128, activation='relu'))\r\n",
        "cnnmodel.add(Dense(LABELS_LEN, activation='softmax'))\r\n",
        "\r\n",
        "cnnmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\r\n",
        "cnnmodel.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5YpYguv9Ght"
      },
      "source": [
        "**LSTM Model with training your own embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9vgcrju9MwO",
        "outputId": "71e42e85-00ef-4419-ae23-983de9a881ab"
      },
      "source": [
        "rnnmodel = Sequential()\r\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\r\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "rnnmodel.add(Dense(2, activation='sigmoid'))\r\n",
        "rnnmodel.compile(loss='binary_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'])\r\n",
        "print('Training the RNN')\r\n",
        "\r\n",
        "rnnmodel.fit(x_train, y_train,\r\n",
        "          batch_size=32,\r\n",
        "          epochs=1,\r\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Training the RNN\n",
            "198/198 [==============================] - 290s 1s/step - loss: 0.4286 - accuracy: 0.7985 - val_loss: 0.2342 - val_accuracy: 0.9028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8660376668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh3UtuygC8Za"
      },
      "source": [
        "**Bidirectional LSTM with 3 Output Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoxGjIY_DDD8",
        "outputId": "d27d3dd1-6dae-4d72-b060-6e498d69c818"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(embedding_layer)\r\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)))\r\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)))\r\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(64)))\r\n",
        "model.add(Dense(64, activation='relu', kernel_initializer='uniform'))\r\n",
        "model.add(Dense(2, activation='sigmoid'))\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data= (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 341s 243ms/step - loss: 0.6434 - accuracy: 0.6196 - val_loss: 0.4753 - val_accuracy: 0.7807\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 307s 245ms/step - loss: 0.4436 - accuracy: 0.7985 - val_loss: 0.3591 - val_accuracy: 0.8463\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 308s 246ms/step - loss: 0.3555 - accuracy: 0.8458 - val_loss: 0.3241 - val_accuracy: 0.8578\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 308s 246ms/step - loss: 0.3066 - accuracy: 0.8651 - val_loss: 0.3039 - val_accuracy: 0.8712\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 308s 246ms/step - loss: 0.2812 - accuracy: 0.8783 - val_loss: 0.3238 - val_accuracy: 0.8569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe6defa8e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zSg2ZG9Goqv"
      },
      "source": [
        "test = ['it was a very bad movie']\r\n",
        "test = tokenizer.texts_to_sequences(test)\r\n",
        "test = pad_sequences(test, maxlen=train_df['review'].shape[0], dtype='int32', value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4AmoyevMSDq",
        "outputId": "8eff20d6-125f-46ec-dc11-020ea4e1acf4"
      },
      "source": [
        "sentiment = model.predict(test)[0]\r\n",
        "if (np.argmax(sentiment) == 0):\r\n",
        "  print('Negative')\r\n",
        "elif (np.argmax(sentiment) == 1):\r\n",
        "  print('positive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIyhAQrQNuZf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}